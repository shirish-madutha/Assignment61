{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b9140d-4d44-482a-9058-0eec84805be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q1. What is Gradient Boosting Regression? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Gradient Boosting Regression is a machine learning algorithm that belongs to the ensemble learning family. It is used for regression tasks, where the goal is to predict a continuous numerical value. The algorithm builds an ensemble of weak learners (usually decision trees) sequentially, with each new learner correcting the errors of the combined ensemble so far. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017b1f6-a6f0-4725-9818-0f601ce5fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared. \"\"\"\n",
    "\n",
    "\"\"\" Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\"  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dbcd585-729c-4256-b347-df311fbf1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'n_estimators': 100, 'tree_depth': 4}\n",
      "Best Mean Squared Error: 0.1396849096241534\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with the mean of the target variable\n",
    "        predictions = np.full(y.shape, np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the residuals\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            # Fit a weak learner (decision tree) to the residuals\n",
    "            tree = self._fit_tree(X, residuals)\n",
    "            \n",
    "            # Update predictions based on the weak learner\n",
    "            predictions += self.learning_rate * self._predict_tree(tree, X)\n",
    "\n",
    "            # Save the weak learner\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def _fit_tree(self, X, y):\n",
    "        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        tree.fit(X, y)\n",
    "        return tree\n",
    "\n",
    "    def _predict_tree(self, tree, X):\n",
    "        return tree.predict(X).reshape(-1, 1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using all weak learners\n",
    "        predictions = np.sum(self.learning_rate * self._predict_tree(tree, X) for tree in self.models)\n",
    "        return predictions\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 4 * (X - 0.5) ** 2 + np.random.randn(100, 1) / 10\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize hyperparameters\n",
    "learning_rates = [0.01, 0.1, 0.2]\n",
    "n_estimators_list = [50, 100, 200]\n",
    "tree_depths = [3, 4, 5]\n",
    "\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "# Perform grid search\n",
    "for lr in learning_rates:\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for tree_depth in tree_depths:\n",
    "            # Create and train the model\n",
    "            model = GradientBoostingRegressor(learning_rate=lr, n_estimators=n_estimators, max_depth=tree_depth)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate the model\n",
    "            y_pred = model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "            # Update the best parameters if the current model is better\n",
    "            if mse < best_score:\n",
    "                best_score = mse\n",
    "                best_params = {'learning_rate': lr, 'n_estimators': n_estimators, 'tree_depth': tree_depth}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Mean Squared Error:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df43258-28d0-4f96-b3b9-c517cfad036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q4. What is a weak learner in Gradient Boosting? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" A weak learner in the context of Gradient Boosting is a model that performs slightly better than random chance. Typically, decision trees with shallow depth are used as weak learners in gradient boosting algorithms. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89486ef-f885-4166-a271-32adf6c116ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q5. What is the intuition behind the Gradient Boosting algorithm? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" The intuition behind Gradient Boosting is to sequentially add models to the ensemble, where each model corrects the errors of the combined ensemble so far. This is achieved by fitting a weak learner to the residuals (the differences between the actual and predicted values) of the current ensemble. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b0c1b-57b2-4948-be8a-a1ebadd32d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q6. How does Gradient Boosting algorithm build an ensemble of weak learners? \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" Gradient Boosting builds an ensemble of weak learners sequentially. It starts with an initial prediction (often the mean of the target variable) and then fits a weak learner to the residuals (errors) of the current prediction. The subsequent weak learners are trained to predict the negative gradient of the loss function with respect to the ensemble's current prediction. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739303c-bffa-4f49-a626-5e28a623f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?  \"\"\"\n",
    "\n",
    "# ans\n",
    "\"\"\" \n",
    "Initialize with a Constant Value: The algorithm starts with a constant prediction, often the mean of the target variable.\n",
    "\n",
    "Compute Residuals: Compute the residuals by subtracting the current prediction from the actual target values.\n",
    "\n",
    "Fit a Weak Learner to Residuals: Train a weak learner (e.g., a decision tree) on the residuals. The goal is to capture the patterns that the current ensemble fails to predict.\n",
    "\n",
    "Compute Learning Rate Multiplier: Multiply the predictions of the weak learner by a small value (learning rate) before adding them to the current ensemble. This controls the step size in the direction of minimizing the loss.\n",
    "\n",
    "Update Predictions: Update the ensemble predictions by adding the learning rate multiplied predictions of the weak learner.\n",
    "\n",
    "Repeat: Repeat steps 2-5 until a predefined number of weak learners are built or a convergence criterion is met.\n",
    "\n",
    "The final prediction is the sum of the predictions from all the weak learners. The process minimizes a predefined loss function, effectively improving the model's performance. \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
